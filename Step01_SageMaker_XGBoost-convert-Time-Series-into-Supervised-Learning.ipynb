{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用SageMaker+XGBoost，将时间序列转换为监督学习，完成预测性维护的实践\n",
    "https://github.com/liangyimingcom/Use-SageMaker_XGBoost-convert-Time-Series-into-Supervised-Learning-for-predictive-maintenance\n",
    "**关键字**：SageMaker；XGBoost；Python；滑窗；滑动窗口方法；时间序列预测转化为监督学习问题；将多元时间序列数据转换为监督学习问题；如何用Python将时间序列问题转化为有监督学习问题；时间序列预测的机器学习模型；Machine Learning；ML；\n",
    "\n",
    "# Step1: 数据预处理与特征工程，以及滑动窗口原理解析\n",
    "\n",
    "**《预测性维护》是传统制造业常见AI场景。过去多年，制造业一直在努力提高运营效率，并避免由于组件故障而导致停机。通常使用的方法是：**\n",
    "1. 通常采用的方法是使用“物理传感器（标签）”做数据连接，存储和大屏上进行了大量重复投资，以监视设备状况并获得实时警报。\n",
    "2. 主要的数据分析方法是单变量阈值和基于物理的建模方法，尽管这些方法在检测特定故障类型和操作条件方面很有效，但它们通常会错过\"通过推导每台设备的多元关系\"而检测到的重要信息。\n",
    "3. 借助机器学习，可以提供从设备的历史数据中学习的数据驱动模型。主要挑战在于，Machine Learning(ML)的项目投资和工程师培训，实施这样的机器学习解决方案既耗时又昂贵。\n",
    "\n",
    "**AWS Sagemaker提供了一个简单有效的解决方案，就是使用Sagemaker+XGboost完成检测到异常的设备行为，实现《预测性维护》的场景需求**，本文内容覆盖了：\n",
    "1. 使用了“**滑窗**”方法进行数据集的重构，并配合XGBoost算法，**将多元时间序列数据集转换为监督学习问题（复杂问题转换为简单问题）；**\n",
    "2. 使用Sagemaker Studio各项功能（自动机器学习Autopilot、自动化的调参 Hyperparameter tuning jobs、多模型终端节点multi-model endpoints等）**加速XGBoost超参数优化的速度，有效提高模型准确度，并大幅降低日程推理成本**；\n",
    "3. 使用Sagemaker Studio **完成数据预处理与特征工程**：\n",
    "   - [ ] 1）探索相关性；\n",
    "   - [ ] 2）缩小特征值范围；\n",
    "   - [ ] 3）将海量数据分为几批进行预处理，以避免服务器内存溢出；\n",
    "   - [ ] 4）数据清理，滑动窗口清除无效数据；\n",
    "   - [ ] 5）过滤数据，解决正负样本不平衡的问题；\n",
    "4. 针对实验数据，使用Sagemaker+XGboost训练了6个预测模型，分别覆盖提前5、10、20、30、40、50分钟进行预测，演示预测结果结果。\n",
    "\n",
    "---\n",
    "### 本章节内容Contents\n",
    "\n",
    "1. [数据预处理与特征工程]\n",
    "2. [滑动窗口的代码实现]\n",
    "3. [数据预处理]\n",
    "4. [样本不均衡处理]\n",
    "5. [数据标注与特征工程]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slidingwindow === 10, earlywarningcycle === 6\n"
     ]
    }
   ],
   "source": [
    "## [滑窗值：合并多少条记录在一起] 【真实结果为 n+1 】\n",
    "n_slidingwindow = 10  #n_slidingwindow = 100\n",
    "## [提前n个周期预警, 分钟 = n/2 分钟预警（30s一个间隔）] ，应该小于滑窗值\n",
    "n_earlywarningcycle = 6\n",
    "\n",
    "#周期预警的开始位\n",
    "n_combinrows = n_earlywarningcycle\n",
    "print('slidingwindow === ' + str(n_slidingwindow) +  ', earlywarningcycle === ' + str(n_earlywarningcycle) )\n",
    "\n",
    "## 共计58个属性column\n",
    "n_totalcolumns = 58 \n",
    "\n",
    "## load csv path\n",
    "csv_path = './yiming-arraged/10000871-part_ac.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "#from xgboost import XGBRegressor\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb-10000871-part_ac.csv-TC58-SW010-WC006-20210405135629\n"
     ]
    }
   ],
   "source": [
    "# 预定义的环境变量\n",
    "s3_prefix_mask = 'xgb-{}-TC{}-SW{}-WC{}-{}' # haierkong-xgb-{filename}-TC_{total columns 58}-SW_{n_slidingwindow 60}-WC_{warningcycle 30}-{created datetime}\n",
    "#print(s3_prefix_mask) #for testing\n",
    "\n",
    "csv_path_head, csv_path_tail = os.path.split(csv_path)\n",
    "#created_date = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "created_date = strftime(\"%Y%m%d%H%M%S\", gmtime())\n",
    "\n",
    "strfilled_n_slidingwindow=str(n_slidingwindow).rjust(3,'0')\n",
    "strfilled_n_earlywarningcycle=str(n_earlywarningcycle).rjust(3,'0')\n",
    "s3_prefix = s3_prefix_mask.format(csv_path_tail, n_totalcolumns, strfilled_n_slidingwindow, strfilled_n_earlywarningcycle, created_date)\n",
    "print(s3_prefix) #for testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path,low_memory=False)\n",
    "\n",
    "#df n_totalcolumns = 58 columns\n",
    "df = df[[\n",
    "'1S0Z7F92N86S68KI_1_AUXILIQUIDOPENING',\n",
    "'1S0Z7F92N86S68KI_1_COMPCURRENT',\n",
    "'1S0Z7F92N86S68KI_1_COMPEXHAUSTTEMP',\n",
    "'1S0Z7F92N86S68KI_1_COMPLOAD',\n",
    "'1S0Z7F92N86S68KI_1_COMPPOWER',\n",
    "'1S0Z7F92N86S68KI_1_COMPRUNTIME',\n",
    "'1S0Z7F92N86S68KI_1_COMPSPEED',\n",
    "'1S0Z7F92N86S68KI_1_COMPSUCTIONTEMP',\n",
    "'1S0Z7F92N86S68KI_1_COMPVOLTAGE',\n",
    "'1S0Z7F92N86S68KI_1_CONDSIDEEXHAUSTPRESS',\n",
    "'1S0Z7F92N86S68KI_1_DISCHARGESUPERHEAT',\n",
    "'1S0Z7F92N86S68KI_1_ECONPRESS',\n",
    "'1S0Z7F92N86S68KI_1_ECONREFRTEMP',\n",
    "'1S0Z7F92N86S68KI_1_EVAPSIDESUCTIONPRESS',\n",
    "'1S0Z7F92N86S68KI_1_INVERTERTEMP',\n",
    "'1S0Z7F92N86S68KI_1_MAINFLOWVALVEOPENING',\n",
    "'1S0Z7F92N86S68KI_1_MAINLOOPLEVEL',\n",
    "'1S0Z7F92N86S68KI_2_AUXILIQUIDOPENING',\n",
    "'1S0Z7F92N86S68KI_2_COMPCURRENT',\n",
    "'1S0Z7F92N86S68KI_2_COMPEXHAUSTTEMP',\n",
    "'1S0Z7F92N86S68KI_2_COMPLOAD',\n",
    "'1S0Z7F92N86S68KI_2_COMPPOWER',\n",
    "'1S0Z7F92N86S68KI_2_COMPRUNTIME',\n",
    "'1S0Z7F92N86S68KI_2_COMPSPEED',\n",
    "'1S0Z7F92N86S68KI_2_COMPSUCTIONTEMP',\n",
    "'1S0Z7F92N86S68KI_2_COMPVOLTAGE',\n",
    "'1S0Z7F92N86S68KI_2_CONDSIDEEXHAUSTPRESS',\n",
    "'1S0Z7F92N86S68KI_2_DISCHARGESUPERHEAT',\n",
    "'1S0Z7F92N86S68KI_2_ECONPRESS',\n",
    "'1S0Z7F92N86S68KI_2_ECONREFRTEMP',\n",
    "'1S0Z7F92N86S68KI_2_EVAPSIDESUCTIONPRESS',\n",
    "'1S0Z7F92N86S68KI_2_INVERTERTEMP',\n",
    "'1S0Z7F92N86S68KI_2_MAINFLOWVALVEOPENING',\n",
    "'1S0Z7F92N86S68KI_2_MAINLOOPLEVEL',\n",
    "'1S0Z7F92N86S68KI_3_AUXILIQUIDOPENING',\n",
    "'1S0Z7F92N86S68KI_3_COMPCURRENT',\n",
    "'1S0Z7F92N86S68KI_3_COMPEXHAUSTTEMP',\n",
    "'1S0Z7F92N86S68KI_3_COMPLOAD',\n",
    "'1S0Z7F92N86S68KI_3_COMPPOWER',\n",
    "'1S0Z7F92N86S68KI_3_COMPRUNTIME',\n",
    "'1S0Z7F92N86S68KI_3_COMPSPEED',\n",
    "'1S0Z7F92N86S68KI_3_COMPSUCTIONTEMP',\n",
    "'1S0Z7F92N86S68KI_3_COMPVOLTAGE',\n",
    "'1S0Z7F92N86S68KI_3_CONDSIDEEXHAUSTPRESS',\n",
    "'1S0Z7F92N86S68KI_3_DISCHARGESUPERHEAT',\n",
    "'1S0Z7F92N86S68KI_3_ECONPRESS',\n",
    "'1S0Z7F92N86S68KI_3_ECONREFRTEMP',\n",
    "'1S0Z7F92N86S68KI_3_EVAPSIDESUCTIONPRESS',\n",
    "'1S0Z7F92N86S68KI_3_INVERTERTEMP',\n",
    "'1S0Z7F92N86S68KI_3_MAINFLOWVALVEOPENING',\n",
    "'1S0Z7F92N86S68KI_3_MAINLOOPLEVEL',\n",
    "'SYSTEM_CONDCAPACITY',\n",
    "'SYSTEM_CONDSIDETEMPIN',\n",
    "'SYSTEM_CONDSIDETEMPOUT',\n",
    "'SYSTEM_EVAPCAPACITY',\n",
    "'SYSTEM_EVAPSIDETEMPOUT',\n",
    "'SYSTEM_UNITPOWER',\n",
    "'code','time'\n",
    "]]\n",
    "\n",
    "#df #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据清理0： 排序\n",
    "# 按照时间排序数组后，然后删除时间的属性\n",
    "df['time']=pd.to_datetime(df['time'])\n",
    "df.sort_values('time', inplace=True)\n",
    "# save for time sort checking...\n",
    "#df.to_csv('modeldata_sortbytime.csv', header=True, index=True) #for testing\n",
    "df = df.drop(['time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据清理0: 转化 code 到 hascode = true/false \n",
    "# 根据hascode属性的请看，添加Ishascode =true/false 的列\n",
    "df['hascode'] = (np.where(df['code'].isnull().values, False, True)).astype(object)\n",
    "df = df.drop(['code'], axis=1)\n",
    "#df.info()  # for testing\n",
    "#df.head(3)  # for testing\n",
    "\n",
    "##数据清理0:  将 “Ishascode =true/false”的列置换到第一列，作为分类Lable\n",
    "model_data = pd.get_dummies(df)\n",
    "model_data = pd.concat([model_data['hascode_True'], model_data.drop(\n",
    "    ['hascode_False', 'hascode_True'], axis=1)], axis=1)\n",
    "#willdel#model_data_values = model_data_values[0:10, 13:15] ## for testing\n",
    "#willdel#DataFrame(model_data_values).info()\n",
    "#model_data #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数 #\n",
    "\n",
    "# 时间序列数据集转换为监督学习问题，将《多列时间序列数据》转换为《监督学习问题》Transform the timeseries data into supervised learning\n",
    "# 参数: data=原始数据集；n_in=滑窗值（合并多少条时序记录合并在一起）；dropnan=是否保留华创后部分为空的记录；\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "       n_vars = 1 if type(data) is list else data.shape[1]\n",
    "       df = DataFrame(data)\n",
    "       cols = list()\n",
    "        \n",
    "       # input sequence (t-n, ... t-1)\n",
    "       for i in range(n_in, 0, -1):\n",
    "              cols.append(df.shift(i))\n",
    "                \n",
    "       # forecast sequence (t, t+1, ... t+n)\n",
    "       for i in range(0, n_out):\n",
    "              cols.append(df.shift(-i))\n",
    "                \n",
    "       # put it all together\n",
    "       agg = concat(cols, axis=1)\n",
    "        \n",
    "       # drop rows with NaN values\n",
    "       if dropnan:\n",
    "              agg.dropna(inplace=True)\n",
    "       return agg.values\n",
    "\n",
    "\n",
    "# 对数据集进行分段滑窗，从而避免内存溢出；\n",
    "# 参数：data=原始数据集；n_in=滑窗值（合并多少条时序记录合并在一起）；splite_md=行分段的数量，分的越小内存占用越小\n",
    "def splite_series_to_supervised(data, n_in=1, splite_md=500000):\n",
    "    splited_series_to_supervised = pd.DataFrame()  #定义一个临时的dataframe，用于解决内存溢出的问题\n",
    "    # start, stop, step 三个参数可以为负数\n",
    "    for i in range(0,len(data),splite_md):\n",
    "        print('/ total= '+str(len(data))+', now splited_working_number= ' + str(i)), # for testing    \n",
    "        splited_series_to_supervised = splited_series_to_supervised.append(DataFrame(\n",
    "            series_to_supervised(model_data.iloc[ i : i+splite_md ], n_in=n_in, dropnan=False)))\n",
    "        #print(model_data.iloc[ i : i+splite_md ].info()) # for testing\n",
    "    \n",
    "    return splited_series_to_supervised\n",
    "\n",
    "\n",
    "#滑窗后处理：滑窗后的数据清理，将不是最后错误之前发生的滑窗全部删除\n",
    "# 用 n_slidingwindow（滑窗数量） 做一个循环：不考虑（查询）最后一位的hascode，前面所有hascode=1 (每间隔58个列的第一个是hascode=true/false)的行全部删除；\n",
    "# 即：只保留全部hascode=0的行(没有错误发生的行) 与 最后一位hascode=1的行(第一个错误发生的行)\n",
    "def clear_supervised(data, n_slidingwindow, n_totalcolumns):\n",
    "    count = 0\n",
    "    while count < n_slidingwindow :\n",
    "        n_colnum = count * n_totalcolumns\n",
    "        data.drop(index=data[data[n_colnum].isin([True])].index, inplace=True)\n",
    "        print('/ total = '+str(n_slidingwindow) +', now working on = ' + str(count)), # for testing\n",
    "        #print('column num= '+ str(n_colnum)) # for testing\n",
    "        count = count + 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "#-----------------------#-----------------------#-----------------------\n",
    "\n",
    "# 滑窗后处理：正确滑窗，应该是 “有错和无错，各自一条”； 同时适用于pd.sample随机\n",
    "#即：挑出报错时的最后一条数据 + 删除上面N条未报错数据（上一条正常数据为行数为 except - n）\n",
    "def pickup_supervised_4train(data, n_slidingwindow, n_totalcolumns, splite_md=500000):\n",
    "    splited_series_to_supervised = pd.DataFrame()  #定义一个临时的dataframe，用于解决内存溢出的问题\n",
    "    \n",
    "    for i in range(0,len(data),splite_md):\n",
    "        print('\\nworking_number ==' + str(i)), # for testing    \n",
    "\n",
    "        splited_data= data.iloc[ i : i+splite_md ]\n",
    "        splited_data.reset_index(drop=True,inplace=True)\n",
    "        print('/ splited_data ==== '+str(splited_data.shape[0])),\n",
    "\n",
    "        n_checkpoint = n_totalcolumns * n_slidingwindow # 检查点位数 \n",
    "        index_hascode_truerows = splited_data[splited_data[n_checkpoint].isin([True])].index #检查点列 为真的Index号，用于下一步挑出来\n",
    "        print('/ index_hascode_truerows ====== '+str(index_hascode_truerows)),\n",
    "\n",
    "        target_data = pd.DataFrame(data=splited_data,index=index_hascode_truerows) # 把检查点列 为真 挑出来\n",
    "        target_data = target_data.append(pd.DataFrame(data=splited_data,index=index_hascode_truerows-n_slidingwindow)) # 【-滑窗数量可以取到完全不同的validi值，适用于sample随机】把检查点列 为真的上n行，挑出来 (N等于滑窗个数)\n",
    "        target_data[n_checkpoint].fillna(0, inplace=True) #上一行的 检查点列 有可能是空的，空值清洗为0\n",
    "        print('/ target_data ======== '+ str(target_data.shape[0])),\n",
    "\n",
    "        splited_series_to_supervised = splited_series_to_supervised.append(target_data)\n",
    "\n",
    "    print('\\n========================================')\n",
    "    print('splited_series_to_supervised  ============================== '+ str(splited_series_to_supervised.shape[0])) # for testing1\n",
    "    splited_series_to_supervised.drop_duplicates(inplace=True) #清除重复的行（造成1增加）\n",
    "    return splited_series_to_supervised\n",
    "\n",
    "# 滑窗后处理：正确滑窗，应该是 “有错和无错，各自一条”； 同时适用于pd.sample随机\n",
    "# 即：挑出报错时的最后一条数据 + 删除上面N条未报错数据（上一条正常数据为行数为 except - n）\n",
    "# 样本不均衡处理：以状态位=1的row为准，向上画出一个状态位=0的矩阵，从而仅保留部分状态位=0的滑窗集合（非故障数据集的筛选）\n",
    "def pickup_supervised_4train_imbalance(data, n_slidingwindow, n_totalcolumns, splite_md=500000):\n",
    "    splited_series_to_supervised = pd.DataFrame()  #定义一个临时的dataframe，用于解决内存溢出的问题\n",
    "    \n",
    "    for i in range(0,len(data),splite_md):\n",
    "        print('\\nworking_number ==' + str(i)), # for testing    \n",
    "\n",
    "        splited_data= data.iloc[ i : i+splite_md ]\n",
    "        splited_data.reset_index(drop=True,inplace=True)\n",
    "        print('/ splited_data ==== '+str(splited_data.shape[0])),\n",
    "\n",
    "        n_checkpoint = n_totalcolumns * n_slidingwindow # 检查点位数 \n",
    "        index_hascode_truerows = splited_data[splited_data[n_checkpoint].isin([True])].index #检查点列 为真的Index号，用于下一步挑出来\n",
    "        print('/ index_hascode_truerows ====== '+str(index_hascode_truerows)),\n",
    "\n",
    "        target_data = pd.DataFrame(data=splited_data,index=index_hascode_truerows) # 把检查点列 为真 挑出来\n",
    "        \n",
    "          #--- 以状态位=1的row为准，向上画出一个状态位=0的矩阵 ---# \n",
    "        for i in range(1,n_slidingwindow+1): \n",
    "            target_data = target_data.append(pd.DataFrame(data=splited_data,index=index_hascode_truerows-i)) # 把检查点列 为真的上n滑窗行，挑出来 (N等于滑窗个数)，不适用于sample随机\n",
    "            #print(' DivRows=' + str(i)), #for testing\n",
    "            \n",
    "        target_data[n_checkpoint].fillna(0, inplace=True) #上一行的 检查点列 有可能是空的，空值清洗为0\n",
    "        print('/ target_data ======== '+ str(target_data.shape[0])),\n",
    "\n",
    "        splited_series_to_supervised = splited_series_to_supervised.append(target_data)\n",
    "\n",
    "    print('\\n========================================')\n",
    "    print('splited_series_to_supervised  ============================== '+ str(splited_series_to_supervised.shape[0])) # for testing1\n",
    "    splited_series_to_supervised.drop_duplicates(inplace=True) #清除重复的行（造成1增加）    \n",
    "    return splited_series_to_supervised\n",
    "\n",
    "#-----------------------#-----------------------#-----------------------\n",
    "\n",
    "\n",
    "# 滑窗后处理：准备xgboost的Lable数据集，处理：\n",
    "# 1）将最后的hasissuce code lable放入第一列； \n",
    "# 2）删除合并后最后一行row的信息（共计58个属性column）\n",
    "def ready4inference_supervised(data, n_slidingwindow, n_totalcolumns, n_combinrows):\n",
    "    #两个工作：1）把最后的一位 hascode=true/false 挪到了第一位作为lable； 2）删除最后的58个属性列；    \n",
    "    n_lasthascodepoint = n_slidingwindow*n_totalcolumns\n",
    "    print('n_lasthascodepoint=' + str(n_lasthascodepoint))\n",
    "    \n",
    "    n_drop_end = (n_slidingwindow + 1) * n_totalcolumns # 需要增加58个column才是end；\n",
    "    print('n_drop_end=' + str(n_drop_end))\n",
    "    \n",
    "    n_drop_start = (n_slidingwindow - n_combinrows) * n_totalcolumns\n",
    "    print('n_drop_start=' + str(n_drop_start))\n",
    "    \n",
    "    data = pd.concat([data[n_lasthascodepoint], \n",
    "                            data.drop(data.iloc[:, n_drop_start:n_drop_end], axis=1)], axis=1) \n",
    "    print('\\nFINALL data  ============================== '+ str(data.shape[0])) # for testing\n",
    "\n",
    "    return data\n",
    "\n",
    "# inference预测处理：传入modeldata数据和 sagemaker inference handle，获得预测结果\n",
    "def sagemaker_predict(data, xgb_predictor, rows=1000):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join(\n",
    "            [predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ total= 500000, now splited_working_number= 0\n",
      "/ total= 500000, now splited_working_number= 50000\n",
      "/ total= 500000, now splited_working_number= 100000\n",
      "/ total= 500000, now splited_working_number= 150000\n",
      "/ total= 500000, now splited_working_number= 200000\n",
      "/ total= 500000, now splited_working_number= 250000\n",
      "/ total= 500000, now splited_working_number= 300000\n",
      "/ total= 500000, now splited_working_number= 350000\n",
      "/ total= 500000, now splited_working_number= 400000\n",
      "/ total= 500000, now splited_working_number= 450000\n"
     ]
    }
   ],
   "source": [
    "#数据滑窗 1 ： 分段滑窗，避免内存溢出\n",
    "## transform the timeseries data into supervised learning\n",
    "#旧方法存档#model_data = DataFrame(series_to_supervised(model_data, n_in=n_slidingwindow, dropnan=False))  #合并n行到一行\n",
    "#旧方法存档#model_data.info()   \n",
    "model_data = splite_series_to_supervised(model_data, n_slidingwindow, 50000)\n",
    "\n",
    "#model_data.to_csv('spliteseriestosupervised_data.csv', header=False, index=False) # has NO header for xgboost bin:log\n",
    "#model_data.info() #for testing\n",
    "#model_data #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ total = 10, now working on = 0\n",
      "/ total = 10, now working on = 1\n",
      "/ total = 10, now working on = 2\n",
      "/ total = 10, now working on = 3\n",
      "/ total = 10, now working on = 4\n",
      "/ total = 10, now working on = 5\n",
      "/ total = 10, now working on = 6\n",
      "/ total = 10, now working on = 7\n",
      "/ total = 10, now working on = 8\n",
      "/ total = 10, now working on = 9\n",
      "\n",
      "After clear_supervised model_data issue rows on point = 580\n",
      "\n",
      "After clear_supervised model_data total  ============================== \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 248150 entries, 0 to 47960\n",
      "Columns: 638 entries, 0 to 637\n",
      "dtypes: float64(638)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "# 滑窗后数据处理 2，清理不符合要求的滑入数据；\n",
    "model_data = clear_supervised(model_data, n_slidingwindow, n_totalcolumns)\n",
    "#model_data.to_csv('clearsupervised_data.csv', header=False, index=False) # has NO header for xgboost bin:log\n",
    "\n",
    "# 校验查询用\n",
    "n_checkpoint = n_totalcolumns * n_slidingwindow # 检查点位数  \n",
    "print('\\nAfter clear_supervised model_data issue rows on point = ' + str(n_checkpoint))\n",
    "#model_data[model_data[n_checkpoint].isin([True])].info()  ## 校验查询用 #for testing\n",
    "print('\\nAfter clear_supervised model_data total  ============================== ')\n",
    "model_data.info()  ## 校验查询用 #for testing\n",
    "#model_data # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "working_number ==0\n",
      "/ splited_data ==== 50000\n",
      "/ index_hascode_truerows ====== Int64Index([  338,   756,  1448,  1486,  1539,  1634,  1635,  1640,  2453,\n",
      "             2628,  7272,  8763,  8773, 11000, 12093, 12102, 12191, 12200,\n",
      "            12213, 14575, 14586, 14587, 48678, 48717, 48723, 48735, 49200,\n",
      "            49730, 49920, 49940],\n",
      "           dtype='int64')\n",
      "/ target_data ======== 330\n",
      "\n",
      "working_number ==50000\n",
      "/ splited_data ==== 50000\n",
      "/ index_hascode_truerows ====== Int64Index([   77,   147,   161,   233,   235,   313,   326,   331,   415,\n",
      "              496,\n",
      "            ...\n",
      "            46972, 47025, 47061, 47068, 47138, 47806, 48389, 48427, 49657,\n",
      "            49703],\n",
      "           dtype='int64', length=187)\n",
      "/ target_data ======== 2057\n",
      "\n",
      "working_number ==100000\n",
      "/ splited_data ==== 50000\n",
      "/ index_hascode_truerows ====== Int64Index([  599,  3724,  5379,  5896, 11325, 11330, 13648, 13702, 13920,\n",
      "            13934, 13936, 14001, 14049, 14378, 14404, 14409, 15371, 15557,\n",
      "            15565, 15626, 18048, 18289, 18293, 18295, 20362, 22695, 33829,\n",
      "            33833],\n",
      "           dtype='int64')\n",
      "/ target_data ======== 308\n",
      "\n",
      "working_number ==150000\n",
      "/ splited_data ==== 50000\n",
      "/ index_hascode_truerows ====== Int64Index([23705], dtype='int64')\n",
      "/ target_data ======== 11\n",
      "\n",
      "working_number ==200000\n",
      "/ splited_data ==== 48150\n",
      "/ index_hascode_truerows ====== Int64Index([43165], dtype='int64')\n",
      "/ target_data ======== 11\n",
      "\n",
      "========================================\n",
      "splited_series_to_supervised  ============================== 2717\n",
      "\n",
      "FINALL clear_supervised model_data issue rows on point = 580\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 247 entries, 338 to 43165\n",
      "Columns: 638 entries, 0 to 637\n",
      "dtypes: float64(638)\n",
      "memory usage: 1.2 MB\n",
      "\n",
      "FINALL model_data  ============================== 2511\n"
     ]
    }
   ],
   "source": [
    "#滑窗后数据处理 3： 分段挑选有效数据，避免内存溢出\n",
    "model_data = pickup_supervised_4train_imbalance(model_data,n_slidingwindow, n_totalcolumns, splite_md=50000)\n",
    "#model_data.to_csv('pickupsmallsupervisedfortrain_data.csv', header=False, index=False) # has NO header for xgboost bin:log\n",
    "\n",
    "print('\\nFINALL clear_supervised model_data issue rows on point = ' + str(n_checkpoint))\n",
    "model_data[model_data[n_checkpoint].isin([True])].info()  ## 校验查询用 #for testing\n",
    "print('\\nFINALL model_data  ============================== '+ str(model_data.shape[0])) # for testing\n",
    "#model_data.info()  ## 校验查询用 #for testing\n",
    "#model_data #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_lasthascodepoint=580\n",
      "n_drop_end=638\n",
      "n_drop_start=232\n",
      "\n",
      "FINALL data  ============================== 2511\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2511 entries, 338 to 43155\n",
      "Columns: 233 entries, 580 to 231\n",
      "dtypes: float64(233)\n",
      "memory usage: 4.5 MB\n"
     ]
    }
   ],
   "source": [
    "#滑窗后数据处理 4：准备xgboost training lable的标志位\n",
    "model_data = ready4inference_supervised(model_data,n_slidingwindow, n_totalcolumns,n_combinrows)\n",
    "#model_data.to_csv('ready4inference_data.csv', header=True, index=True)  #for testing\n",
    "model_data.info()  ## 校验查询用 #for testing\n",
    "\n",
    "# for testing\n",
    "#model_data.columns\n",
    "#model_data.columns.values\n",
    "#model_data.head(3)\n",
    "#model_data #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_data, validation_data = np.split(model_data.sample(frac=1,random_state=1729), [int(0.7 * len(model_data))])  #随机的sample，train/vali = 70/30比例\n",
    "train_data, validation_data = np.split(model_data, [int(0.8 * len(model_data))])  ##顺序的sample，train/vali = 70/30比例\n",
    "#train_data.to_csv('train_data.csv', header=True, index=False) #has header #header for autoML on sagemaker\n",
    "train_data.to_csv('train_data.csv', header=False, index=False) # has NO header for xgboost bin:log\n",
    "validation_data.to_csv('validation_data.csv', header=False, index=False)\n",
    "\n",
    "whole_data = model_data\n",
    "whole_data.to_csv('whole_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 3.0M Apr  5 13:56 whole_data.csv\n",
      "-rw-r--r-- 1 root root 605K Apr  5 13:56 validation_data.csv\n",
      "-rw-r--r-- 1 root root 2.4M Apr  5 13:56 train_data.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lht *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "# s3_prefix = 'haier-xgb-{}-SW_{}-WC_{}-{}' #for testing\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(s3_prefix, 'train/train_data.csv')).upload_file('train_data.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(\n",
    "    s3_prefix, 'validation/validation_data.csv')).upload_file('validation_data.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(\n",
    "    s3_prefix, 'wholedata/whole_data.csv')).upload_file('whole_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload completed in: xgb-10000871-part_ac.csv-TC58-SW010-WC006-20210405135629\n",
      "upload completed in bucket: sagemaker-cn-northwest-1-337575217701\n"
     ]
    }
   ],
   "source": [
    "print('upload completed in: ' + s3_prefix)\n",
    "print('upload completed in bucket: ' +bucket)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (arn:aws:sagemaker:cn-northwest-1:390780980154:image/datascience-1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:cn-northwest-1:390780980154:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
